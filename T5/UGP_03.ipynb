{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1f80a3d05274155a35f9def87ed8348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbd18c87c04c45a5a019a9eb55ec3e8d",
              "IPY_MODEL_b4ee180a669d4461b7dd553116797cc6",
              "IPY_MODEL_d669babde89e4300a91836538ae75c52"
            ],
            "layout": "IPY_MODEL_c39e56a0a6a1439082de1f3cde5b9a67"
          }
        },
        "bbd18c87c04c45a5a019a9eb55ec3e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b806d943f9bc4c0bb94cf7c1af2fb319",
            "placeholder": "​",
            "style": "IPY_MODEL_384eb483f76342cc85feea6086586644",
            "value": "100%"
          }
        },
        "b4ee180a669d4461b7dd553116797cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bd35cae3d6e4058a4025a2f39b20787",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_386f48b4cd1e4dc39709712a875ed074",
            "value": 3
          }
        },
        "d669babde89e4300a91836538ae75c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e25a74da0f8443869a91c812a2579ef3",
            "placeholder": "​",
            "style": "IPY_MODEL_25e6f3546adf43b7b6f1101819797abd",
            "value": " 3/3 [00:00&lt;00:00, 82.57it/s]"
          }
        },
        "c39e56a0a6a1439082de1f3cde5b9a67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b806d943f9bc4c0bb94cf7c1af2fb319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384eb483f76342cc85feea6086586644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bd35cae3d6e4058a4025a2f39b20787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "386f48b4cd1e4dc39709712a875ed074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e25a74da0f8443869a91c812a2579ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e6f3546adf43b7b6f1101819797abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEgpjpKJA9kF",
        "outputId": "ad465cb4-86b6-479e-d8b4-7ee8d7c2b930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.23.1\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (4.9.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.1) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, xxhash, urllib3, portalocker, multiprocess, colorama, sacrebleu, responses, huggingface-hub, transformers, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed colorama-0.4.6 datasets-2.9.0 evaluate-0.4.0 huggingface-hub-0.12.0 multiprocess-0.70.14 portalocker-2.7.0 responses-0.18.0 sacrebleu-2.3.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.23.1 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install 'transformers==4.23.1' torch sentencepiece datasets evaluate sacrebleu scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd \"/content/drive/MyDrive/rotten-tomatoes\""
      ],
      "metadata": {
        "id": "QgZjStvBIlt-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "eS-fc8fGyAOW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!python prepare_tomatoes.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F2GQzW3Gitl",
        "outputId": "81f28dfb-6580-42c2-d70b-b78055a525c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading builder script: 100% 5.03k/5.03k [00:00<00:00, 2.71MB/s]\n",
            "Downloading metadata: 100% 2.02k/2.02k [00:00<00:00, 1.95MB/s]\n",
            "Downloading readme: 100% 7.25k/7.25k [00:00<00:00, 6.12MB/s]\n",
            "Downloading and preparing dataset rotten_tomatoes/default to /root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46...\n",
            "Downloading data: 100% 488k/488k [00:00<00:00, 63.8MB/s]\n",
            "Dataset rotten_tomatoes downloaded and prepared to /root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 262.71it/s]\n",
            "Saving into: data/train.json\n",
            "Saving into: data/s2s-train.json\n",
            "Saving into: data/validation.json\n",
            "Saving into: data/s2s-validation.json\n",
            "Saving into: data/test.json\n",
            "Saving into: data/s2s-test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_translation.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path \"google/t5-v1_1-base\" \\\n",
        "  --train_file data/s2s-train.json \\\n",
        "  --validation_file data/s2s-validation.json \\\n",
        "  --test_file data/s2s-test.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --source_lang \"text\" \\\n",
        "  --target_lang \"label\" \\\n",
        "  --source_prefix \"tomatoes classification\" \\\n",
        "  --max_source_length 256 \\\n",
        "  --max_target_length 128 \\\n",
        "  --generation_max_length 128 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --predict_with_generate \\\n",
        "  --max_eval_samples 2000 \\\n",
        "  --num_train_epochs 5 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 250 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 100 \\\n",
        "  --eval_steps 250 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model accuracy \\\n",
        "  --greater_is_better True \\\n",
        "  --load_best_model_at_end True \\\n",
        "  --output_dir out/tomatoes/t5_v1_1_freeze_base"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai_ubFlUGirK",
        "outputId": "f57e293a-8d91-48dc-9407-87998c737390"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-14 21:05:45.251582: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-14 21:05:46.657296: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 21:05:46.657410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-14 21:05:46.657430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=250,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=128,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/tomatoes/t5_v1_1_freeze_base/runs/Feb14_21-05-48_603ca86ea461,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=out/tomatoes/t5_v1_1_freeze_base,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/tomatoes/t5_v1_1_freeze_base,\n",
            "save_on_each_node=False,\n",
            "save_steps=250,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-e2d7b68a3e39120c\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Generating dataset json (/content/.cache_training/json/default-e2d7b68a3e39120c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "Downloading and preparing dataset json/default to /content/.cache_training/json/default-e2d7b68a3e39120c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 13751.82it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2223.13it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /content/.cache_training/json/default-e2d7b68a3e39120c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 998.33it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 605/605 [00:00<00:00, 104kB/s]\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 21:05:51,158 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 21:05:51,162 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 1.86k/1.86k [00:00<00:00, 698kB/s]\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 21:05:51,888 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 21:05:51,889 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 2.30MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 1.79k/1.79k [00:00<00:00, 594kB/s]\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 21:05:54,539 >> loading file spiece.model from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 21:05:54,539 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 21:05:54,539 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 21:05:54,539 >> loading file special_tokens_map.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2023-02-14 21:05:54,539 >> loading file tokenizer_config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 21:05:54,540 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 21:05:54,541 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:653] 2023-02-14 21:05:54,604 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "[INFO|configuration_utils.py:705] 2023-02-14 21:05:54,605 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 990M/990M [00:07<00:00, 134MB/s]\n",
            "[INFO|modeling_utils.py:2156] 2023-02-14 21:06:02,600 >> loading weights file pytorch_model.bin from cache at .cache_training/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2606] 2023-02-14 21:06:05,973 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:2614] 2023-02-14 21:06:05,973 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/t5-v1_1-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "INFO:__main__:Using translation prefix: \"tomatoes classification: \"\n",
            "Running tokenizer on train dataset:   0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-e2d7b68a3e39120c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-30a562374cf7a5a9.arrow\n",
            "Running tokenizer on train dataset: 100% 9/9 [00:01<00:00,  6.38ba/s]\n",
            "INFO:__main__:Set 266 samples for negative-class\n",
            "INFO:__main__:Set 266 samples for positive-class\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-e2d7b68a3e39120c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f3024fcea251f23d.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00, 10.20ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /content/.cache_training/json/default-e2d7b68a3e39120c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-265a79ebca13c4a7.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  8.72ba/s]\n",
            "Downloading builder script: 100% 8.15k/8.15k [00:00<00:00, 6.68MB/s]\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 3.56MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1607] 2023-02-14 21:06:16,609 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2023-02-14 21:06:16,609 >>   Num examples = 8530\n",
            "[INFO|trainer.py:1609] 2023-02-14 21:06:16,609 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1610] 2023-02-14 21:06:16,609 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1611] 2023-02-14 21:06:16,609 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1612] 2023-02-14 21:06:16,609 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2023-02-14 21:06:16,609 >>   Total optimization steps = 5335\n",
            "  0% 0/5335 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-02-14 21:06:16,627 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 17.4879, 'learning_rate': 4.906279287722587e-05, 'epoch': 0.09}\n",
            "{'loss': 10.841, 'learning_rate': 4.8125585754451736e-05, 'epoch': 0.19}\n",
            "  5% 250/5335 [01:06<19:53,  4.26it/s][INFO|trainer.py:2907] 2023-02-14 21:07:22,967 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:07:22,967 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:07:22,967 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 17.67it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.52it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:06,  9.76it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:06,  9.69it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.82it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.69it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.50it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.30it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.05it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.18it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:06,  8.03it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  8.49it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  9.25it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.27it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.26it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.29it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.17it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.39it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.41it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.43it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.47it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:03,  9.52it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:03,  9.31it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.34it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.29it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.06it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.27it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.31it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.90it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  8.97it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  9.04it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  9.21it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.14it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  8.83it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  9.10it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  8.89it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  9.13it/s]\u001b[A\n",
            " 67% 45/67 [00:04<00:02,  9.32it/s]\u001b[A\n",
            " 69% 46/67 [00:04<00:02,  9.22it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  9.32it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  9.35it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.26it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  9.04it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.02it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.07it/s]\u001b[A\n",
            " 81% 54/67 [00:05<00:01,  8.89it/s]\u001b[A\n",
            " 82% 55/67 [00:05<00:01,  9.09it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  9.22it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  9.10it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:00,  9.30it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:00,  9.26it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  8.95it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  9.00it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  9.16it/s]\u001b[A\n",
            " 94% 63/67 [00:06<00:00,  8.06it/s]\u001b[A\n",
            " 96% 64/67 [00:06<00:00,  7.63it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  7.43it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  7.02it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  7.06it/s]\u001b[A\n",
            "{'eval_loss': 0.5482303500175476, 'eval_bleu': 0.0, 'eval_accuracy': 0.5, 'eval_gen_len': 2.0, 'eval_runtime': 7.6506, 'eval_samples_per_second': 69.537, 'eval_steps_per_second': 8.757, 'epoch': 0.23}\n",
            "\n",
            "  5% 250/5335 [01:13<19:53,  4.26it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:07:30,619 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:07:30,620 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:07:34,266 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:07:34,267 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:07:34,268 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:07:34,320 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-250/spiece.model\n",
            "{'loss': 4.3578, 'learning_rate': 4.71883786316776e-05, 'epoch': 0.28}\n",
            "{'loss': 1.2112, 'learning_rate': 4.625117150890347e-05, 'epoch': 0.37}\n",
            "{'loss': 0.8717, 'learning_rate': 4.5313964386129336e-05, 'epoch': 0.47}\n",
            "  9% 500/5335 [02:29<20:13,  3.98it/s][INFO|trainer.py:2907] 2023-02-14 21:08:46,387 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:08:46,387 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:08:46,387 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:04, 15.66it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.27it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:06, 10.09it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:05,  9.90it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.52it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:05,  9.55it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.44it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.40it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  8.86it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.09it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  8.77it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  8.78it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  8.49it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:05,  8.41it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  8.80it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:05,  8.99it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:05,  8.01it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:05,  8.26it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:05,  7.91it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:05,  7.92it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  8.32it/s]\u001b[A\n",
            " 40% 27/67 [00:03<00:05,  7.74it/s]\u001b[A\n",
            " 42% 28/67 [00:03<00:04,  7.95it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  8.41it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:04,  8.51it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:04,  8.77it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.06it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  8.76it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.09it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.28it/s]\u001b[A\n",
            " 54% 36/67 [00:04<00:03,  8.79it/s]\u001b[A\n",
            " 55% 37/67 [00:04<00:03,  8.95it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  8.97it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  8.71it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:03,  8.90it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  8.80it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  8.71it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  8.24it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  8.31it/s]\u001b[A\n",
            " 67% 45/67 [00:05<00:02,  8.30it/s]\u001b[A\n",
            " 69% 46/67 [00:05<00:02,  8.21it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  8.44it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  8.71it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:02,  8.39it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  8.53it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  8.57it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  8.19it/s]\u001b[A\n",
            " 79% 53/67 [00:06<00:01,  8.19it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  8.33it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  8.28it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  8.46it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  8.69it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:01,  8.64it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:00,  8.76it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  8.83it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  8.59it/s]\u001b[A\n",
            " 93% 62/67 [00:07<00:00,  8.90it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  9.04it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  9.12it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  9.22it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  8.94it/s]\u001b[A\n",
            "{'eval_loss': 0.3310123383998871, 'eval_bleu': 0.0, 'eval_accuracy': 0.6579, 'eval_gen_len': 2.0, 'eval_runtime': 7.7645, 'eval_samples_per_second': 68.517, 'eval_steps_per_second': 8.629, 'epoch': 0.47}\n",
            "\n",
            "  9% 500/5335 [02:37<20:13,  3.98it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:08:54,152 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:08:54,153 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:08:57,602 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:08:57,603 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:08:57,604 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:08:57,671 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-500/spiece.model\n",
            "{'loss': 0.7822, 'learning_rate': 4.43767572633552e-05, 'epoch': 0.56}\n",
            "{'loss': 0.5247, 'learning_rate': 4.343955014058107e-05, 'epoch': 0.66}\n",
            " 14% 750/5335 [03:53<22:05,  3.46it/s][INFO|trainer.py:2907] 2023-02-14 21:10:10,041 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:10:10,041 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:10:10,041 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 3/67 [00:00<00:04, 14.48it/s]\u001b[A\n",
            "  7% 5/67 [00:00<00:05, 11.17it/s]\u001b[A\n",
            " 10% 7/67 [00:00<00:05, 10.54it/s]\u001b[A\n",
            " 13% 9/67 [00:00<00:05, 10.44it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:05, 10.24it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.44it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.43it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  8.75it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  8.83it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  8.56it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:06,  7.92it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  8.27it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:05,  8.45it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:05,  8.58it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:05,  8.34it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  8.69it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  8.61it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:05,  7.67it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  8.09it/s]\u001b[A\n",
            " 42% 28/67 [00:03<00:04,  8.46it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  8.09it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:04,  8.37it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:04,  8.54it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:04,  8.72it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  8.90it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  8.50it/s]\u001b[A\n",
            " 54% 36/67 [00:04<00:03,  8.39it/s]\u001b[A\n",
            " 55% 37/67 [00:04<00:03,  8.63it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  8.09it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  8.04it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:03,  8.09it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:03,  8.03it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  8.84it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  8.83it/s]\u001b[A\n",
            " 67% 45/67 [00:05<00:02,  9.09it/s]\u001b[A\n",
            " 69% 46/67 [00:05<00:02,  9.14it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  9.22it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  9.10it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.03it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  9.00it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.36it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.39it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  9.18it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  9.18it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  9.14it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  9.28it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:01,  8.96it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  9.36it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  9.08it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  9.28it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  9.15it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  9.24it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  9.37it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  9.40it/s]\u001b[A\n",
            "{'eval_loss': 0.36008161306381226, 'eval_bleu': 0.0, 'eval_accuracy': 0.4981, 'eval_gen_len': 2.0, 'eval_runtime': 7.619, 'eval_samples_per_second': 69.826, 'eval_steps_per_second': 8.794, 'epoch': 0.7}\n",
            "\n",
            " 14% 750/5335 [04:01<22:05,  3.46it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:10:17,662 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:10:17,662 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:10:21,144 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:10:21,145 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:10:21,146 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:10:21,264 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-750/spiece.model\n",
            "{'loss': 0.5609, 'learning_rate': 4.250234301780694e-05, 'epoch': 0.75}\n",
            "{'loss': 0.4362, 'learning_rate': 4.15651358950328e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4412, 'learning_rate': 4.0627928772258675e-05, 'epoch': 0.94}\n",
            " 19% 1000/5335 [05:17<21:33,  3.35it/s][INFO|trainer.py:2907] 2023-02-14 21:11:33,872 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:11:33,872 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:11:33,872 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 18.56it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.34it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.20it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:06,  9.36it/s]\u001b[A\n",
            " 15% 10/67 [00:01<00:06,  9.39it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:05,  9.35it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.35it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.34it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.31it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.29it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.18it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  8.71it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  8.73it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:05,  8.96it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  9.04it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.27it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.36it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.13it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.22it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.28it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.00it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.10it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.06it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  9.08it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:04,  9.11it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.11it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.12it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.00it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  8.98it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.00it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.66it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  8.85it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  8.90it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  9.00it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:03,  8.90it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  8.87it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  9.25it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  9.28it/s]\u001b[A\n",
            " 67% 45/67 [00:04<00:02,  8.96it/s]\u001b[A\n",
            " 69% 46/67 [00:04<00:02,  9.09it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  9.25it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  9.33it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.00it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  8.84it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  8.30it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  8.07it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  8.26it/s]\u001b[A\n",
            " 81% 54/67 [00:05<00:01,  8.05it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  8.33it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  8.59it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  8.63it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:01,  6.25it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:01,  5.02it/s]\u001b[A\n",
            " 90% 60/67 [00:07<00:01,  4.79it/s]\u001b[A\n",
            " 91% 61/67 [00:07<00:01,  5.01it/s]\u001b[A\n",
            " 93% 62/67 [00:07<00:00,  5.30it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  5.42it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  5.17it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  5.40it/s]\u001b[A\n",
            " 99% 66/67 [00:08<00:00,  6.19it/s]\u001b[A\n",
            "100% 67/67 [00:08<00:00,  6.96it/s]\u001b[A\n",
            "{'eval_loss': 0.33371853828430176, 'eval_bleu': 0.0, 'eval_accuracy': 0.6391, 'eval_gen_len': 2.0, 'eval_runtime': 8.3429, 'eval_samples_per_second': 63.767, 'eval_steps_per_second': 8.031, 'epoch': 0.94}\n",
            "\n",
            " 19% 1000/5335 [05:25<21:33,  3.35it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:11:42,216 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:11:42,217 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:11:46,366 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:11:46,367 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:11:46,367 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:11:46,427 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-1000/spiece.model\n",
            "{'loss': 0.4469, 'learning_rate': 3.9690721649484535e-05, 'epoch': 1.03}\n",
            "{'loss': 0.4093, 'learning_rate': 3.87535145267104e-05, 'epoch': 1.12}\n",
            " 23% 1250/5335 [06:43<18:17,  3.72it/s][INFO|trainer.py:2907] 2023-02-14 21:13:00,252 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:13:00,252 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:13:00,252 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 18.97it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 12.10it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.30it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:07,  8.03it/s]\u001b[A\n",
            " 13% 9/67 [00:00<00:07,  8.24it/s]\u001b[A\n",
            " 15% 10/67 [00:01<00:06,  8.48it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:06,  8.51it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:06,  8.70it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:06,  7.82it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:07,  6.88it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:06,  7.49it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:06,  7.93it/s]\u001b[A\n",
            " 25% 17/67 [00:02<00:06,  8.02it/s]\u001b[A\n",
            " 27% 18/67 [00:02<00:05,  8.35it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:05,  8.61it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  8.89it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:05,  9.15it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.30it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.09it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.08it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  8.74it/s]\u001b[A\n",
            " 39% 26/67 [00:03<00:04,  8.40it/s]\u001b[A\n",
            " 40% 27/67 [00:03<00:04,  8.70it/s]\u001b[A\n",
            " 42% 28/67 [00:03<00:04,  8.92it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  8.99it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:04,  9.11it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.24it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.31it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.44it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.40it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.53it/s]\u001b[A\n",
            " 54% 36/67 [00:04<00:03,  8.78it/s]\u001b[A\n",
            " 55% 37/67 [00:04<00:03,  8.91it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  8.97it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  8.98it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.10it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  9.03it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  9.26it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  9.31it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  9.40it/s]\u001b[A\n",
            " 67% 45/67 [00:05<00:02,  8.96it/s]\u001b[A\n",
            " 69% 46/67 [00:05<00:02,  9.16it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  9.28it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  8.98it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.00it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  9.13it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  9.27it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.57it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  9.33it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  9.35it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  9.15it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:00,  9.50it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:00,  9.43it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  9.28it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  9.29it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  9.39it/s]\u001b[A\n",
            " 94% 63/67 [00:06<00:00,  9.39it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  9.18it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  9.29it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  9.37it/s]\u001b[A\n",
            "{'eval_loss': 0.3354308605194092, 'eval_bleu': 0.0, 'eval_accuracy': 0.5921, 'eval_gen_len': 2.0, 'eval_runtime': 7.5983, 'eval_samples_per_second': 70.016, 'eval_steps_per_second': 8.818, 'epoch': 1.17}\n",
            "\n",
            " 23% 1250/5335 [06:51<18:17,  3.72it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:13:07,851 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-1250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:13:07,852 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:13:11,310 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:13:11,311 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:13:11,312 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:13:11,384 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-1250/spiece.model\n",
            "{'loss': 0.4177, 'learning_rate': 3.7816307403936275e-05, 'epoch': 1.22}\n",
            "{'loss': 0.4164, 'learning_rate': 3.6879100281162135e-05, 'epoch': 1.31}\n",
            "{'loss': 0.4321, 'learning_rate': 3.594189315838801e-05, 'epoch': 1.41}\n",
            " 28% 1500/5335 [08:08<16:05,  3.97it/s][INFO|trainer.py:2907] 2023-02-14 21:14:24,998 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:14:24,998 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:14:24,998 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 17.93it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.93it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.43it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:05, 10.14it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.82it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.69it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.45it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.50it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.13it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.07it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  9.11it/s]\u001b[A\n",
            " 28% 19/67 [00:01<00:05,  9.39it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:04,  9.41it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.54it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.57it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.48it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.49it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.32it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.43it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.58it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.46it/s]\u001b[A\n",
            " 43% 29/67 [00:02<00:03,  9.54it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:03,  9.40it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.38it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.49it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.44it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.40it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.11it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.67it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  8.94it/s]\u001b[A\n",
            " 57% 38/67 [00:03<00:03,  9.00it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  9.12it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.06it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  8.86it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  9.06it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  8.91it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  8.82it/s]\u001b[A\n",
            " 67% 45/67 [00:04<00:02,  9.04it/s]\u001b[A\n",
            " 69% 46/67 [00:04<00:02,  9.12it/s]\u001b[A\n",
            " 70% 47/67 [00:04<00:02,  9.17it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.32it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  9.37it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  9.43it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.45it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.30it/s]\u001b[A\n",
            " 81% 54/67 [00:05<00:01,  9.04it/s]\u001b[A\n",
            " 82% 55/67 [00:05<00:01,  9.10it/s]\u001b[A\n",
            " 84% 56/67 [00:05<00:01,  9.18it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  9.29it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:00,  9.22it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:00,  9.19it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  9.24it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  9.05it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  9.05it/s]\u001b[A\n",
            " 94% 63/67 [00:06<00:00,  8.91it/s]\u001b[A\n",
            " 96% 64/67 [00:06<00:00,  9.12it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  9.57it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  9.67it/s]\u001b[A\n",
            "{'eval_loss': 0.33724361658096313, 'eval_bleu': 0.0, 'eval_accuracy': 0.5789, 'eval_gen_len': 2.0, 'eval_runtime': 7.2735, 'eval_samples_per_second': 73.142, 'eval_steps_per_second': 9.212, 'epoch': 1.41}\n",
            "\n",
            " 28% 1500/5335 [08:15<16:05,  3.97it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:14:32,272 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:14:32,273 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:14:35,753 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:14:35,754 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:14:35,754 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:14:35,827 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-1500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:14:43,651 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-250] due to args.save_total_limit\n",
            "{'loss': 0.4083, 'learning_rate': 3.500468603561387e-05, 'epoch': 1.5}\n",
            "{'loss': 0.4001, 'learning_rate': 3.406747891283974e-05, 'epoch': 1.59}\n",
            " 33% 1750/5335 [09:37<17:43,  3.37it/s][INFO|trainer.py:2907] 2023-02-14 21:15:53,878 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:15:53,878 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:15:53,878 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:06,  9.64it/s]\u001b[A\n",
            "  4% 3/67 [00:00<00:08,  7.51it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:09,  6.65it/s]\u001b[A\n",
            "  7% 5/67 [00:00<00:09,  6.24it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:10,  5.56it/s]\u001b[A\n",
            " 10% 7/67 [00:01<00:10,  5.56it/s]\u001b[A\n",
            " 12% 8/67 [00:01<00:10,  5.39it/s]\u001b[A\n",
            " 13% 9/67 [00:01<00:10,  5.46it/s]\u001b[A\n",
            " 15% 10/67 [00:01<00:10,  5.18it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:10,  5.20it/s]\u001b[A\n",
            " 18% 12/67 [00:02<00:11,  4.89it/s]\u001b[A\n",
            " 19% 13/67 [00:02<00:12,  4.40it/s]\u001b[A\n",
            " 21% 14/67 [00:02<00:11,  4.75it/s]\u001b[A\n",
            " 22% 15/67 [00:02<00:10,  4.87it/s]\u001b[A\n",
            " 24% 16/67 [00:03<00:11,  4.54it/s]\u001b[A\n",
            " 25% 17/67 [00:03<00:12,  4.12it/s]\u001b[A\n",
            " 27% 18/67 [00:03<00:10,  4.59it/s]\u001b[A\n",
            " 28% 19/67 [00:03<00:09,  4.88it/s]\u001b[A\n",
            " 30% 20/67 [00:03<00:09,  4.91it/s]\u001b[A\n",
            " 31% 21/67 [00:04<00:08,  5.29it/s]\u001b[A\n",
            " 33% 22/67 [00:04<00:08,  5.40it/s]\u001b[A\n",
            " 34% 23/67 [00:04<00:08,  5.33it/s]\u001b[A\n",
            " 36% 24/67 [00:04<00:07,  5.65it/s]\u001b[A\n",
            " 37% 25/67 [00:04<00:09,  4.35it/s]\u001b[A\n",
            " 39% 26/67 [00:05<00:10,  3.84it/s]\u001b[A\n",
            " 40% 27/67 [00:05<00:12,  3.13it/s]\u001b[A\n",
            " 42% 28/67 [00:06<00:12,  3.07it/s]\u001b[A\n",
            " 43% 29/67 [00:06<00:10,  3.73it/s]\u001b[A\n",
            " 45% 30/67 [00:06<00:08,  4.27it/s]\u001b[A\n",
            " 46% 31/67 [00:06<00:07,  4.78it/s]\u001b[A\n",
            " 48% 32/67 [00:06<00:06,  5.35it/s]\u001b[A\n",
            " 49% 33/67 [00:06<00:05,  5.83it/s]\u001b[A\n",
            " 51% 34/67 [00:06<00:05,  6.12it/s]\u001b[A\n",
            " 52% 35/67 [00:07<00:05,  6.39it/s]\u001b[A\n",
            " 54% 36/67 [00:07<00:04,  6.52it/s]\u001b[A\n",
            " 55% 37/67 [00:07<00:04,  6.70it/s]\u001b[A\n",
            " 57% 38/67 [00:07<00:04,  6.30it/s]\u001b[A\n",
            " 58% 39/67 [00:07<00:04,  6.25it/s]\u001b[A\n",
            " 60% 40/67 [00:07<00:04,  6.16it/s]\u001b[A\n",
            " 61% 41/67 [00:08<00:04,  5.95it/s]\u001b[A\n",
            " 63% 42/67 [00:08<00:04,  5.82it/s]\u001b[A\n",
            " 64% 43/67 [00:08<00:04,  5.90it/s]\u001b[A\n",
            " 66% 44/67 [00:08<00:03,  5.78it/s]\u001b[A\n",
            " 67% 45/67 [00:08<00:03,  5.88it/s]\u001b[A\n",
            " 69% 46/67 [00:08<00:03,  5.96it/s]\u001b[A\n",
            " 70% 47/67 [00:08<00:03,  6.60it/s]\u001b[A\n",
            " 73% 49/67 [00:09<00:02,  7.59it/s]\u001b[A\n",
            " 76% 51/67 [00:09<00:01,  8.36it/s]\u001b[A\n",
            " 78% 52/67 [00:09<00:01,  8.49it/s]\u001b[A\n",
            " 79% 53/67 [00:09<00:01,  8.72it/s]\u001b[A\n",
            " 81% 54/67 [00:09<00:01,  8.64it/s]\u001b[A\n",
            " 82% 55/67 [00:09<00:01,  8.83it/s]\u001b[A\n",
            " 84% 56/67 [00:09<00:01,  8.99it/s]\u001b[A\n",
            " 85% 57/67 [00:10<00:01,  9.09it/s]\u001b[A\n",
            " 88% 59/67 [00:10<00:00,  9.47it/s]\u001b[A\n",
            " 90% 60/67 [00:10<00:00,  9.34it/s]\u001b[A\n",
            " 91% 61/67 [00:10<00:00,  9.29it/s]\u001b[A\n",
            " 93% 62/67 [00:10<00:00,  9.11it/s]\u001b[A\n",
            " 94% 63/67 [00:10<00:00,  9.22it/s]\u001b[A\n",
            " 96% 64/67 [00:10<00:00,  9.41it/s]\u001b[A\n",
            " 97% 65/67 [00:10<00:00,  9.49it/s]\u001b[A\n",
            " 99% 66/67 [00:11<00:00,  9.47it/s]\u001b[A\n",
            "100% 67/67 [00:11<00:00,  9.56it/s]\u001b[A\n",
            "{'eval_loss': 0.33486971259117126, 'eval_bleu': 0.0, 'eval_accuracy': 0.5902, 'eval_gen_len': 2.0, 'eval_runtime': 11.3425, 'eval_samples_per_second': 46.903, 'eval_steps_per_second': 5.907, 'epoch': 1.64}\n",
            "\n",
            " 33% 1750/5335 [09:48<17:43,  3.37it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:16:05,222 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-1750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:16:05,223 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:16:08,713 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:16:08,714 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:16:08,714 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-1750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:16:08,767 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-1750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:16:16,603 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-750] due to args.save_total_limit\n",
            "{'loss': 0.4085, 'learning_rate': 3.31302717900656e-05, 'epoch': 1.69}\n",
            "{'loss': 0.3862, 'learning_rate': 3.2193064667291475e-05, 'epoch': 1.78}\n",
            "{'loss': 0.3819, 'learning_rate': 3.125585754451734e-05, 'epoch': 1.87}\n",
            " 37% 2000/5335 [11:02<13:00,  4.27it/s][INFO|trainer.py:2907] 2023-02-14 21:17:19,245 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:17:19,245 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:17:19,245 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 18.25it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.45it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.45it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:05, 10.20it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05, 10.20it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.92it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.81it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.70it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.66it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  9.48it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:06,  7.32it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:07,  6.03it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:06,  7.26it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:05,  7.71it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:05,  8.00it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:05,  8.22it/s]\u001b[A\n",
            " 37% 25/67 [00:03<00:08,  5.13it/s]\u001b[A\n",
            " 39% 26/67 [00:03<00:06,  5.87it/s]\u001b[A\n",
            " 40% 27/67 [00:03<00:06,  6.61it/s]\u001b[A\n",
            " 42% 28/67 [00:03<00:06,  6.37it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:05,  6.56it/s]\u001b[A\n",
            " 45% 30/67 [00:04<00:10,  3.64it/s]\u001b[A\n",
            " 46% 31/67 [00:04<00:09,  3.65it/s]\u001b[A\n",
            " 48% 32/67 [00:04<00:08,  4.23it/s]\u001b[A\n",
            " 49% 33/67 [00:04<00:07,  4.75it/s]\u001b[A\n",
            " 51% 34/67 [00:04<00:06,  5.16it/s]\u001b[A\n",
            " 52% 35/67 [00:05<00:05,  5.60it/s]\u001b[A\n",
            " 54% 36/67 [00:05<00:05,  6.00it/s]\u001b[A\n",
            " 55% 37/67 [00:05<00:04,  6.16it/s]\u001b[A\n",
            " 57% 38/67 [00:05<00:04,  6.01it/s]\u001b[A\n",
            " 58% 39/67 [00:05<00:04,  6.14it/s]\u001b[A\n",
            " 60% 40/67 [00:05<00:04,  6.22it/s]\u001b[A\n",
            " 61% 41/67 [00:05<00:04,  6.49it/s]\u001b[A\n",
            " 63% 42/67 [00:06<00:03,  6.52it/s]\u001b[A\n",
            " 64% 43/67 [00:06<00:03,  6.36it/s]\u001b[A\n",
            " 66% 44/67 [00:06<00:03,  6.14it/s]\u001b[A\n",
            " 67% 45/67 [00:06<00:03,  6.07it/s]\u001b[A\n",
            " 69% 46/67 [00:06<00:03,  5.89it/s]\u001b[A\n",
            " 70% 47/67 [00:06<00:03,  5.96it/s]\u001b[A\n",
            " 72% 48/67 [00:07<00:03,  5.94it/s]\u001b[A\n",
            " 73% 49/67 [00:07<00:03,  5.99it/s]\u001b[A\n",
            " 75% 50/67 [00:07<00:02,  6.01it/s]\u001b[A\n",
            " 76% 51/67 [00:07<00:02,  6.72it/s]\u001b[A\n",
            " 78% 52/67 [00:07<00:02,  7.27it/s]\u001b[A\n",
            " 79% 53/67 [00:07<00:01,  7.70it/s]\u001b[A\n",
            " 81% 54/67 [00:07<00:01,  7.97it/s]\u001b[A\n",
            " 82% 55/67 [00:08<00:01,  8.40it/s]\u001b[A\n",
            " 84% 56/67 [00:08<00:01,  8.64it/s]\u001b[A\n",
            " 85% 57/67 [00:08<00:01,  8.78it/s]\u001b[A\n",
            " 88% 59/67 [00:08<00:00,  9.29it/s]\u001b[A\n",
            " 90% 60/67 [00:08<00:00,  9.13it/s]\u001b[A\n",
            " 91% 61/67 [00:08<00:00,  9.12it/s]\u001b[A\n",
            " 93% 62/67 [00:08<00:00,  9.12it/s]\u001b[A\n",
            " 94% 63/67 [00:08<00:00,  9.31it/s]\u001b[A\n",
            " 96% 64/67 [00:08<00:00,  9.40it/s]\u001b[A\n",
            " 97% 65/67 [00:09<00:00,  9.39it/s]\u001b[A\n",
            "100% 67/67 [00:09<00:00,  9.71it/s]\u001b[A\n",
            "{'eval_loss': 0.3342953324317932, 'eval_bleu': 0.0, 'eval_accuracy': 0.5771, 'eval_gen_len': 2.0, 'eval_runtime': 9.4328, 'eval_samples_per_second': 56.399, 'eval_steps_per_second': 7.103, 'epoch': 1.87}\n",
            "\n",
            " 37% 2000/5335 [11:12<13:00,  4.27it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:17:28,679 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-2000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:17:28,680 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:17:32,149 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:17:32,150 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:17:32,151 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:17:32,199 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-2000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:17:40,017 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.3831, 'learning_rate': 3.0318650421743205e-05, 'epoch': 1.97}\n",
            "{'loss': 0.3915, 'learning_rate': 2.9381443298969075e-05, 'epoch': 2.06}\n",
            " 42% 2250/5335 [12:24<11:54,  4.32it/s][INFO|trainer.py:2907] 2023-02-14 21:18:41,184 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:18:41,184 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:18:41,184 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 17.63it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.75it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.52it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:05,  9.87it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.86it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.48it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.35it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.14it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  8.96it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  8.81it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  8.85it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  9.09it/s]\u001b[A\n",
            " 28% 19/67 [00:01<00:05,  9.22it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  9.03it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.28it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.40it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.19it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.29it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.12it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.05it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.17it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.21it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:03,  9.38it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.10it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.23it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.29it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.41it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.24it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.78it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  8.92it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  8.97it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  9.03it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.16it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  8.91it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  9.07it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  9.31it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  8.95it/s]\u001b[A\n",
            " 67% 45/67 [00:04<00:02,  8.32it/s]\u001b[A\n",
            " 69% 46/67 [00:05<00:02,  7.48it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  7.22it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  6.92it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:02,  6.96it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:02,  7.00it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:02,  6.91it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:02,  6.88it/s]\u001b[A\n",
            " 79% 53/67 [00:06<00:02,  6.98it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  6.90it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  6.97it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  6.83it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  6.46it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:01,  6.31it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:01,  6.64it/s]\u001b[A\n",
            " 90% 60/67 [00:07<00:01,  6.72it/s]\u001b[A\n",
            " 91% 61/67 [00:07<00:00,  6.81it/s]\u001b[A\n",
            " 93% 62/67 [00:07<00:00,  6.74it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  6.82it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  6.47it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  6.46it/s]\u001b[A\n",
            " 99% 66/67 [00:08<00:00,  6.12it/s]\u001b[A\n",
            "100% 67/67 [00:08<00:00,  6.20it/s]\u001b[A\n",
            "{'eval_loss': 0.3317296802997589, 'eval_bleu': 0.0, 'eval_accuracy': 0.6598, 'eval_gen_len': 2.0, 'eval_runtime': 8.4096, 'eval_samples_per_second': 63.261, 'eval_steps_per_second': 7.967, 'epoch': 2.11}\n",
            "\n",
            " 42% 2250/5335 [12:32<11:54,  4.32it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:18:49,596 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-2250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:18:49,597 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:18:53,139 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:18:53,140 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:18:53,140 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:18:53,185 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-2250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:19:01,036 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.3953, 'learning_rate': 2.8444236176194938e-05, 'epoch': 2.16}\n",
            "{'loss': 0.3766, 'learning_rate': 2.7507029053420808e-05, 'epoch': 2.25}\n",
            "{'loss': 0.3864, 'learning_rate': 2.6569821930646678e-05, 'epoch': 2.34}\n",
            " 47% 2500/5335 [13:46<10:45,  4.39it/s][INFO|trainer.py:2907] 2023-02-14 21:20:03,335 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:20:03,335 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:20:03,335 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 19.94it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 12.16it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.44it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:05, 10.00it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.70it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.52it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.37it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.31it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.45it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.34it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  9.23it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  9.15it/s]\u001b[A\n",
            " 28% 19/67 [00:01<00:05,  9.23it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  9.30it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.28it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.41it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.24it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.19it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.27it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.43it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.19it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  9.09it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:04,  9.11it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.31it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.15it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.05it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.21it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.20it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.75it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  8.79it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  8.69it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  8.98it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.06it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  8.99it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  9.07it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  9.28it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  9.16it/s]\u001b[A\n",
            " 67% 45/67 [00:04<00:02,  9.06it/s]\u001b[A\n",
            " 69% 46/67 [00:04<00:02,  8.62it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  8.57it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  8.64it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:02,  8.70it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  8.64it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.23it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.32it/s]\u001b[A\n",
            " 81% 54/67 [00:05<00:01,  9.12it/s]\u001b[A\n",
            " 82% 55/67 [00:05<00:01,  9.22it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  8.88it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  8.78it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:00,  9.00it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:00,  8.76it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  7.86it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  7.03it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  7.07it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  6.98it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  6.93it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  7.13it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  7.06it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  7.25it/s]\u001b[A\n",
            "{'eval_loss': 0.3333333432674408, 'eval_bleu': 0.0, 'eval_accuracy': 0.5714, 'eval_gen_len': 2.0, 'eval_runtime': 7.736, 'eval_samples_per_second': 68.77, 'eval_steps_per_second': 8.661, 'epoch': 2.34}\n",
            "\n",
            " 47% 2500/5335 [13:54<10:45,  4.39it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:20:11,073 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-2500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:20:11,075 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:20:14,644 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:20:14,645 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:20:14,645 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:20:14,691 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-2500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:20:22,552 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-1250] due to args.save_total_limit\n",
            "{'loss': 0.3923, 'learning_rate': 2.563261480787254e-05, 'epoch': 2.44}\n",
            "{'loss': 0.394, 'learning_rate': 2.4695407685098408e-05, 'epoch': 2.53}\n",
            " 52% 2750/5335 [15:08<09:59,  4.31it/s][INFO|trainer.py:2907] 2023-02-14 21:21:25,092 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:21:25,092 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:21:25,092 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 17.75it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.88it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.45it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:05, 10.09it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.97it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.73it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.71it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.70it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.67it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.63it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  9.50it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  9.38it/s]\u001b[A\n",
            " 28% 19/67 [00:01<00:05,  9.34it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  9.40it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.45it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.57it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.48it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.51it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.57it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.34it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.44it/s]\u001b[A\n",
            " 43% 29/67 [00:02<00:03,  9.59it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:03,  9.37it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.33it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.46it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.46it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.29it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.26it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.82it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  8.90it/s]\u001b[A\n",
            " 57% 38/67 [00:03<00:03,  8.98it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  9.16it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.07it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  9.00it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  9.08it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  9.17it/s]\u001b[A\n",
            " 69% 46/67 [00:04<00:02,  9.41it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:01,  9.67it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.46it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  9.46it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  9.55it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.44it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.14it/s]\u001b[A\n",
            " 81% 54/67 [00:05<00:01,  9.08it/s]\u001b[A\n",
            " 82% 55/67 [00:05<00:01,  9.00it/s]\u001b[A\n",
            " 84% 56/67 [00:05<00:01,  9.11it/s]\u001b[A\n",
            " 85% 57/67 [00:05<00:01,  9.18it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:01,  8.86it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:00,  9.14it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  9.25it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  9.21it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  9.30it/s]\u001b[A\n",
            " 94% 63/67 [00:06<00:00,  9.27it/s]\u001b[A\n",
            " 96% 64/67 [00:06<00:00,  9.25it/s]\u001b[A\n",
            " 97% 65/67 [00:06<00:00,  9.38it/s]\u001b[A\n",
            " 99% 66/67 [00:06<00:00,  9.52it/s]\u001b[A\n",
            "{'eval_loss': 0.33297088742256165, 'eval_bleu': 0.0, 'eval_accuracy': 0.5827, 'eval_gen_len': 2.0, 'eval_runtime': 7.2329, 'eval_samples_per_second': 73.553, 'eval_steps_per_second': 9.263, 'epoch': 2.58}\n",
            "\n",
            " 52% 2750/5335 [15:15<09:59,  4.31it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:21:32,326 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-2750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:21:32,327 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:21:35,792 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:21:35,793 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:21:35,793 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-2750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:21:35,875 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-2750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:21:43,783 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 0.3758, 'learning_rate': 2.3758200562324275e-05, 'epoch': 2.62}\n",
            "{'loss': 0.3934, 'learning_rate': 2.282099343955014e-05, 'epoch': 2.72}\n",
            "{'loss': 0.3817, 'learning_rate': 2.1883786316776008e-05, 'epoch': 2.81}\n",
            " 56% 3000/5335 [16:29<10:50,  3.59it/s][INFO|trainer.py:2907] 2023-02-14 21:22:45,759 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:22:45,759 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:22:45,760 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:05, 11.07it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:08,  7.46it/s]\u001b[A\n",
            "  7% 5/67 [00:00<00:08,  6.98it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:09,  6.48it/s]\u001b[A\n",
            " 10% 7/67 [00:01<00:09,  6.36it/s]\u001b[A\n",
            " 12% 8/67 [00:01<00:08,  7.09it/s]\u001b[A\n",
            " 13% 9/67 [00:01<00:07,  7.60it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:06,  8.57it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:06,  8.71it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:06,  8.89it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.31it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.36it/s]\u001b[A\n",
            " 25% 17/67 [00:02<00:05,  9.07it/s]\u001b[A\n",
            " 27% 18/67 [00:02<00:05,  9.11it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:05,  9.02it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  8.91it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.56it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.12it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.16it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.36it/s]\u001b[A\n",
            " 39% 26/67 [00:03<00:04,  9.47it/s]\u001b[A\n",
            " 40% 27/67 [00:03<00:04,  9.47it/s]\u001b[A\n",
            " 42% 28/67 [00:03<00:04,  9.42it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  9.47it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:03,  9.42it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.48it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.57it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.64it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.58it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.62it/s]\u001b[A\n",
            " 54% 36/67 [00:04<00:03,  9.02it/s]\u001b[A\n",
            " 55% 37/67 [00:04<00:03,  9.00it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  9.06it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  9.27it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.20it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  9.08it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  9.21it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  9.10it/s]\u001b[A\n",
            " 67% 45/67 [00:05<00:02,  9.42it/s]\u001b[A\n",
            " 69% 46/67 [00:05<00:02,  9.25it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  9.32it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.41it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  9.67it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.70it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.62it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  9.40it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  9.21it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  9.23it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:00,  9.41it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  9.56it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  9.47it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  9.52it/s]\u001b[A\n",
            " 94% 63/67 [00:06<00:00,  9.58it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  9.40it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  9.30it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  9.36it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  9.18it/s]\u001b[A\n",
            "{'eval_loss': 0.333313912153244, 'eval_bleu': 0.0, 'eval_accuracy': 0.5921, 'eval_gen_len': 2.0, 'eval_runtime': 7.655, 'eval_samples_per_second': 69.497, 'eval_steps_per_second': 8.752, 'epoch': 2.81}\n",
            "\n",
            " 56% 3000/5335 [16:36<10:50,  3.59it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:22:53,416 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-3000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:22:53,416 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:22:56,851 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:22:56,852 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:22:56,854 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:22:56,915 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-3000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:23:05,245 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-1750] due to args.save_total_limit\n",
            "{'loss': 0.3874, 'learning_rate': 2.0946579194001875e-05, 'epoch': 2.91}\n",
            "{'loss': 0.3677, 'learning_rate': 2.000937207122774e-05, 'epoch': 3.0}\n",
            " 61% 3250/5335 [17:50<08:26,  4.11it/s][INFO|trainer.py:2907] 2023-02-14 21:24:06,868 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:24:06,868 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:24:06,868 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:04, 13.28it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:07,  8.13it/s]\u001b[A\n",
            "  7% 5/67 [00:00<00:08,  7.67it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:08,  7.15it/s]\u001b[A\n",
            " 10% 7/67 [00:00<00:08,  7.04it/s]\u001b[A\n",
            " 12% 8/67 [00:01<00:08,  6.88it/s]\u001b[A\n",
            " 13% 9/67 [00:01<00:08,  6.64it/s]\u001b[A\n",
            " 15% 10/67 [00:01<00:08,  6.68it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:08,  6.89it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:08,  6.77it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:07,  6.80it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:07,  6.82it/s]\u001b[A\n",
            " 22% 15/67 [00:02<00:07,  6.86it/s]\u001b[A\n",
            " 24% 16/67 [00:02<00:07,  6.50it/s]\u001b[A\n",
            " 25% 17/67 [00:02<00:07,  6.30it/s]\u001b[A\n",
            " 27% 18/67 [00:02<00:08,  6.12it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:07,  6.07it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:07,  6.01it/s]\u001b[A\n",
            " 31% 21/67 [00:03<00:07,  5.93it/s]\u001b[A\n",
            " 33% 22/67 [00:03<00:07,  5.92it/s]\u001b[A\n",
            " 34% 23/67 [00:03<00:07,  5.88it/s]\u001b[A\n",
            " 36% 24/67 [00:03<00:07,  5.95it/s]\u001b[A\n",
            " 37% 25/67 [00:03<00:06,  6.47it/s]\u001b[A\n",
            " 39% 26/67 [00:03<00:05,  7.12it/s]\u001b[A\n",
            " 40% 27/67 [00:03<00:05,  7.79it/s]\u001b[A\n",
            " 42% 28/67 [00:04<00:04,  8.24it/s]\u001b[A\n",
            " 43% 29/67 [00:04<00:04,  8.42it/s]\u001b[A\n",
            " 45% 30/67 [00:04<00:04,  8.62it/s]\u001b[A\n",
            " 48% 32/67 [00:04<00:03,  9.10it/s]\u001b[A\n",
            " 49% 33/67 [00:04<00:03,  9.19it/s]\u001b[A\n",
            " 52% 35/67 [00:04<00:03,  9.13it/s]\u001b[A\n",
            " 54% 36/67 [00:04<00:03,  8.90it/s]\u001b[A\n",
            " 55% 37/67 [00:05<00:03,  9.05it/s]\u001b[A\n",
            " 57% 38/67 [00:05<00:03,  9.07it/s]\u001b[A\n",
            " 58% 39/67 [00:05<00:03,  9.12it/s]\u001b[A\n",
            " 60% 40/67 [00:05<00:02,  9.19it/s]\u001b[A\n",
            " 61% 41/67 [00:05<00:02,  9.08it/s]\u001b[A\n",
            " 64% 43/67 [00:05<00:02,  9.56it/s]\u001b[A\n",
            " 66% 44/67 [00:05<00:02,  9.30it/s]\u001b[A\n",
            " 69% 46/67 [00:06<00:02,  9.49it/s]\u001b[A\n",
            " 70% 47/67 [00:06<00:02,  9.56it/s]\u001b[A\n",
            " 72% 48/67 [00:06<00:02,  9.47it/s]\u001b[A\n",
            " 73% 49/67 [00:06<00:01,  9.12it/s]\u001b[A\n",
            " 75% 50/67 [00:06<00:01,  9.22it/s]\u001b[A\n",
            " 78% 52/67 [00:06<00:01,  9.73it/s]\u001b[A\n",
            " 79% 53/67 [00:06<00:01,  9.45it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  9.28it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  9.26it/s]\u001b[A\n",
            " 84% 56/67 [00:07<00:01,  9.31it/s]\u001b[A\n",
            " 85% 57/67 [00:07<00:01,  9.47it/s]\u001b[A\n",
            " 87% 58/67 [00:07<00:00,  9.29it/s]\u001b[A\n",
            " 88% 59/67 [00:07<00:00,  9.43it/s]\u001b[A\n",
            " 90% 60/67 [00:07<00:00,  9.58it/s]\u001b[A\n",
            " 91% 61/67 [00:07<00:00,  9.45it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  9.68it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  9.43it/s]\u001b[A\n",
            " 97% 65/67 [00:08<00:00,  9.43it/s]\u001b[A\n",
            " 99% 66/67 [00:08<00:00,  9.45it/s]\u001b[A\n",
            "100% 67/67 [00:08<00:00,  9.46it/s]\u001b[A\n",
            "{'eval_loss': 0.3299393653869629, 'eval_bleu': 0.0, 'eval_accuracy': 0.6447, 'eval_gen_len': 2.0, 'eval_runtime': 8.4582, 'eval_samples_per_second': 62.897, 'eval_steps_per_second': 7.921, 'epoch': 3.05}\n",
            "\n",
            " 61% 3250/5335 [17:58<08:26,  4.11it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:24:15,328 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-3250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:24:15,328 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:24:18,795 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:24:18,796 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:24:18,797 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:24:18,842 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-3250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:24:26,704 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 0.3715, 'learning_rate': 1.9072164948453608e-05, 'epoch': 3.09}\n",
            "{'loss': 0.3764, 'learning_rate': 1.8134957825679478e-05, 'epoch': 3.19}\n",
            "{'loss': 0.3719, 'learning_rate': 1.7197750702905344e-05, 'epoch': 3.28}\n",
            " 66% 3500/5335 [19:12<07:07,  4.29it/s][INFO|trainer.py:2907] 2023-02-14 21:25:28,765 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:25:28,765 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:25:28,765 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 3/67 [00:00<00:04, 14.99it/s]\u001b[A\n",
            "  7% 5/67 [00:00<00:05, 11.60it/s]\u001b[A\n",
            " 10% 7/67 [00:00<00:05, 10.81it/s]\u001b[A\n",
            " 13% 9/67 [00:00<00:05, 10.18it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:05, 10.12it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.76it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.78it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.74it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  8.94it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  8.41it/s]\u001b[A\n",
            " 28% 19/67 [00:01<00:06,  8.00it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:06,  7.51it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:06,  7.50it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:06,  7.26it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:06,  7.18it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:05,  7.22it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:06,  6.94it/s]\u001b[A\n",
            " 39% 26/67 [00:03<00:05,  6.91it/s]\u001b[A\n",
            " 40% 27/67 [00:03<00:05,  6.98it/s]\u001b[A\n",
            " 42% 28/67 [00:03<00:05,  6.89it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:05,  6.85it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:05,  6.75it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:05,  6.84it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:05,  6.59it/s]\u001b[A\n",
            " 49% 33/67 [00:04<00:05,  6.43it/s]\u001b[A\n",
            " 51% 34/67 [00:04<00:05,  6.53it/s]\u001b[A\n",
            " 52% 35/67 [00:04<00:04,  6.60it/s]\u001b[A\n",
            " 54% 36/67 [00:04<00:04,  6.43it/s]\u001b[A\n",
            " 55% 37/67 [00:04<00:04,  6.21it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:04,  6.07it/s]\u001b[A\n",
            " 58% 39/67 [00:05<00:04,  5.98it/s]\u001b[A\n",
            " 60% 40/67 [00:05<00:04,  6.01it/s]\u001b[A\n",
            " 61% 41/67 [00:05<00:04,  6.05it/s]\u001b[A\n",
            " 63% 42/67 [00:05<00:04,  6.13it/s]\u001b[A\n",
            " 64% 43/67 [00:05<00:03,  6.25it/s]\u001b[A\n",
            " 66% 44/67 [00:05<00:03,  6.25it/s]\u001b[A\n",
            " 67% 45/67 [00:05<00:03,  6.83it/s]\u001b[A\n",
            " 69% 46/67 [00:06<00:02,  7.51it/s]\u001b[A\n",
            " 72% 48/67 [00:06<00:02,  8.43it/s]\u001b[A\n",
            " 73% 49/67 [00:06<00:02,  8.58it/s]\u001b[A\n",
            " 75% 50/67 [00:06<00:01,  8.90it/s]\u001b[A\n",
            " 76% 51/67 [00:06<00:01,  9.00it/s]\u001b[A\n",
            " 79% 53/67 [00:06<00:01,  9.37it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  8.73it/s]\u001b[A\n",
            " 82% 55/67 [00:07<00:01,  8.95it/s]\u001b[A\n",
            " 84% 56/67 [00:07<00:01,  9.02it/s]\u001b[A\n",
            " 85% 57/67 [00:07<00:01,  9.26it/s]\u001b[A\n",
            " 87% 58/67 [00:07<00:00,  9.45it/s]\u001b[A\n",
            " 88% 59/67 [00:07<00:00,  9.32it/s]\u001b[A\n",
            " 91% 61/67 [00:07<00:00,  9.47it/s]\u001b[A\n",
            " 93% 62/67 [00:07<00:00,  9.48it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  9.45it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  9.51it/s]\u001b[A\n",
            " 97% 65/67 [00:08<00:00,  9.25it/s]\u001b[A\n",
            "100% 67/67 [00:08<00:00,  9.77it/s]\u001b[A\n",
            "{'eval_loss': 0.3300759494304657, 'eval_bleu': 0.0, 'eval_accuracy': 0.6391, 'eval_gen_len': 2.0, 'eval_runtime': 8.4509, 'eval_samples_per_second': 62.952, 'eval_steps_per_second': 7.928, 'epoch': 3.28}\n",
            "\n",
            " 66% 3500/5335 [19:20<07:07,  4.29it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:25:37,217 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-3500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:25:37,218 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:25:40,679 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:25:40,680 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:25:40,680 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:25:40,733 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-3500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:25:48,555 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-2500] due to args.save_total_limit\n",
            "{'loss': 0.3666, 'learning_rate': 1.626054358013121e-05, 'epoch': 3.37}\n",
            "{'loss': 0.3755, 'learning_rate': 1.5323336457357078e-05, 'epoch': 3.47}\n",
            " 70% 3750/5335 [20:33<06:13,  4.24it/s][INFO|trainer.py:2907] 2023-02-14 21:26:50,044 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:26:50,044 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:26:50,044 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 18.36it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 10.73it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:06,  9.86it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:06,  9.34it/s]\u001b[A\n",
            " 13% 9/67 [00:00<00:06,  9.25it/s]\u001b[A\n",
            " 15% 10/67 [00:01<00:06,  9.04it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:06,  9.02it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:06,  9.05it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.18it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.12it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.29it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  9.29it/s]\u001b[A\n",
            " 28% 19/67 [00:01<00:04,  9.65it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:04,  9.67it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.44it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.47it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.39it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.45it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.42it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.33it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.39it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  9.45it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:04,  9.18it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.48it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.46it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.54it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  9.21it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  9.21it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  9.22it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  8.68it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:03,  8.01it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:03,  7.67it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:03,  7.34it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:03,  6.96it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:03,  6.96it/s]\u001b[A\n",
            " 67% 45/67 [00:05<00:03,  7.00it/s]\u001b[A\n",
            " 69% 46/67 [00:05<00:02,  7.02it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  7.04it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  7.05it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:02,  7.03it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:02,  7.15it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:02,  7.19it/s]\u001b[A\n",
            " 78% 52/67 [00:06<00:02,  7.12it/s]\u001b[A\n",
            " 79% 53/67 [00:06<00:01,  7.17it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  6.98it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  6.87it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  6.79it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  6.75it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:01,  6.52it/s]\u001b[A\n",
            " 88% 59/67 [00:07<00:01,  6.17it/s]\u001b[A\n",
            " 90% 60/67 [00:07<00:01,  6.07it/s]\u001b[A\n",
            " 91% 61/67 [00:07<00:00,  6.01it/s]\u001b[A\n",
            " 93% 62/67 [00:07<00:00,  6.04it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  6.10it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  5.96it/s]\u001b[A\n",
            " 97% 65/67 [00:08<00:00,  6.01it/s]\u001b[A\n",
            " 99% 66/67 [00:08<00:00,  5.84it/s]\u001b[A\n",
            "100% 67/67 [00:08<00:00,  6.25it/s]\u001b[A\n",
            "{'eval_loss': 0.3338029980659485, 'eval_bleu': 0.0, 'eval_accuracy': 0.5357, 'eval_gen_len': 2.0, 'eval_runtime': 8.6126, 'eval_samples_per_second': 61.77, 'eval_steps_per_second': 7.779, 'epoch': 3.51}\n",
            "\n",
            " 70% 3750/5335 [20:42<06:13,  4.24it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:26:58,658 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-3750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:26:58,658 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:27:02,121 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:27:02,122 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:27:02,123 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-3750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:27:02,182 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-3750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:27:10,082 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-2750] due to args.save_total_limit\n",
            "{'loss': 0.3621, 'learning_rate': 1.4386129334582943e-05, 'epoch': 3.56}\n",
            "{'loss': 0.3681, 'learning_rate': 1.344892221180881e-05, 'epoch': 3.66}\n",
            "{'loss': 0.3552, 'learning_rate': 1.251171508903468e-05, 'epoch': 3.75}\n",
            " 75% 4000/5335 [21:55<05:17,  4.21it/s][INFO|trainer.py:2907] 2023-02-14 21:28:12,233 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:28:12,233 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:28:12,233 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 17.18it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.15it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:06, 10.04it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:06,  9.68it/s]\u001b[A\n",
            " 15% 10/67 [00:01<00:06,  9.23it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:06,  9.31it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.28it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.24it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.17it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.06it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.10it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  8.91it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  9.00it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:05,  9.20it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  9.33it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.39it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.41it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.33it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.62it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.33it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.35it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  9.46it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:03,  9.28it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.16it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.33it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.19it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.59it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.93it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  9.17it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  9.18it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  9.33it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.26it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  9.13it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  9.09it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  9.40it/s]\u001b[A\n",
            " 67% 45/67 [00:04<00:02,  9.50it/s]\u001b[A\n",
            " 69% 46/67 [00:04<00:02,  9.38it/s]\u001b[A\n",
            " 70% 47/67 [00:04<00:02,  9.46it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.44it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  9.15it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  9.25it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.43it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.41it/s]\u001b[A\n",
            " 81% 54/67 [00:05<00:01,  9.20it/s]\u001b[A\n",
            " 82% 55/67 [00:05<00:01,  9.13it/s]\u001b[A\n",
            " 84% 56/67 [00:05<00:01,  9.13it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  9.32it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:00,  9.33it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:00,  9.32it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  8.64it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  7.89it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  7.61it/s]\u001b[A\n",
            " 94% 63/67 [00:06<00:00,  7.27it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  6.81it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  6.98it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  6.83it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  6.98it/s]\u001b[A\n",
            "{'eval_loss': 0.3309723436832428, 'eval_bleu': 0.0, 'eval_accuracy': 0.5508, 'eval_gen_len': 2.0, 'eval_runtime': 7.6495, 'eval_samples_per_second': 69.547, 'eval_steps_per_second': 8.759, 'epoch': 3.75}\n",
            "\n",
            " 75% 4000/5335 [22:03<05:17,  4.21it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:28:19,884 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-4000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:28:19,885 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:28:23,456 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:28:23,457 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:28:23,457 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:28:23,501 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-4000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:28:31,373 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 0.3655, 'learning_rate': 1.1574507966260544e-05, 'epoch': 3.84}\n",
            "{'loss': 0.3522, 'learning_rate': 1.063730084348641e-05, 'epoch': 3.94}\n",
            " 80% 4250/5335 [23:18<04:46,  3.79it/s][INFO|trainer.py:2907] 2023-02-14 21:29:35,226 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:29:35,227 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:29:35,227 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 19.36it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.33it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.21it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:06,  9.81it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.60it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.50it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.23it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.34it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.48it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  9.29it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  9.18it/s]\u001b[A\n",
            " 28% 19/67 [00:01<00:05,  9.06it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:05,  9.17it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.37it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.43it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.27it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.37it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.18it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.30it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.32it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  9.11it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:04,  9.21it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.22it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.18it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.29it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.28it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.27it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.77it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  8.98it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  9.03it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  8.99it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:02,  9.05it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  8.67it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  8.88it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:02,  9.30it/s]\u001b[A\n",
            " 67% 45/67 [00:04<00:02,  9.20it/s]\u001b[A\n",
            " 69% 46/67 [00:04<00:02,  9.28it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  9.36it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  9.12it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.06it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  9.28it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  9.10it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.26it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.35it/s]\u001b[A\n",
            " 81% 54/67 [00:05<00:01,  9.04it/s]\u001b[A\n",
            " 82% 55/67 [00:05<00:01,  9.14it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  9.06it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  9.03it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:00,  9.30it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  9.18it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  9.11it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  9.27it/s]\u001b[A\n",
            " 94% 63/67 [00:06<00:00,  9.39it/s]\u001b[A\n",
            " 96% 64/67 [00:06<00:00,  9.37it/s]\u001b[A\n",
            " 97% 65/67 [00:06<00:00,  9.54it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  9.46it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  9.46it/s]\u001b[A\n",
            "{'eval_loss': 0.3240150511264801, 'eval_bleu': 0.0, 'eval_accuracy': 0.6673, 'eval_gen_len': 2.0, 'eval_runtime': 7.3442, 'eval_samples_per_second': 72.439, 'eval_steps_per_second': 9.123, 'epoch': 3.98}\n",
            "\n",
            " 80% 4250/5335 [23:25<04:46,  3.79it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:29:42,572 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-4250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:29:42,573 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:29:46,020 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:29:46,022 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:29:46,022 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:29:46,087 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-4250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:29:53,937 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-2250] due to args.save_total_limit\n",
            "{'loss': 0.363, 'learning_rate': 9.700093720712277e-06, 'epoch': 4.03}\n",
            "{'loss': 0.3586, 'learning_rate': 8.762886597938144e-06, 'epoch': 4.12}\n",
            "{'loss': 0.3618, 'learning_rate': 7.825679475164012e-06, 'epoch': 4.22}\n",
            " 84% 4500/5335 [24:39<03:41,  3.77it/s][INFO|trainer.py:2907] 2023-02-14 21:30:56,177 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:30:56,177 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:30:56,177 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:04, 14.09it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:07,  8.41it/s]\u001b[A\n",
            "  7% 5/67 [00:00<00:08,  7.44it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:08,  7.15it/s]\u001b[A\n",
            " 10% 7/67 [00:00<00:08,  6.97it/s]\u001b[A\n",
            " 12% 8/67 [00:01<00:08,  6.94it/s]\u001b[A\n",
            " 13% 9/67 [00:01<00:08,  6.87it/s]\u001b[A\n",
            " 15% 10/67 [00:01<00:08,  6.58it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:08,  6.34it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:08,  6.15it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:08,  6.06it/s]\u001b[A\n",
            " 21% 14/67 [00:02<00:09,  5.76it/s]\u001b[A\n",
            " 22% 15/67 [00:02<00:08,  5.89it/s]\u001b[A\n",
            " 24% 16/67 [00:02<00:08,  5.85it/s]\u001b[A\n",
            " 25% 17/67 [00:02<00:08,  5.73it/s]\u001b[A\n",
            " 27% 18/67 [00:02<00:08,  5.62it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:07,  6.28it/s]\u001b[A\n",
            " 30% 20/67 [00:03<00:06,  6.84it/s]\u001b[A\n",
            " 31% 21/67 [00:03<00:06,  7.20it/s]\u001b[A\n",
            " 33% 22/67 [00:03<00:05,  7.66it/s]\u001b[A\n",
            " 34% 23/67 [00:03<00:05,  7.92it/s]\u001b[A\n",
            " 36% 24/67 [00:03<00:05,  8.15it/s]\u001b[A\n",
            " 37% 25/67 [00:03<00:04,  8.48it/s]\u001b[A\n",
            " 39% 26/67 [00:03<00:04,  8.75it/s]\u001b[A\n",
            " 40% 27/67 [00:03<00:04,  8.51it/s]\u001b[A\n",
            " 42% 28/67 [00:03<00:04,  8.60it/s]\u001b[A\n",
            " 43% 29/67 [00:04<00:04,  8.87it/s]\u001b[A\n",
            " 45% 30/67 [00:04<00:04,  8.80it/s]\u001b[A\n",
            " 46% 31/67 [00:04<00:03,  9.09it/s]\u001b[A\n",
            " 48% 32/67 [00:04<00:03,  9.20it/s]\u001b[A\n",
            " 49% 33/67 [00:04<00:03,  9.07it/s]\u001b[A\n",
            " 51% 34/67 [00:04<00:03,  9.06it/s]\u001b[A\n",
            " 52% 35/67 [00:04<00:03,  8.75it/s]\u001b[A\n",
            " 54% 36/67 [00:04<00:03,  8.38it/s]\u001b[A\n",
            " 55% 37/67 [00:04<00:03,  8.65it/s]\u001b[A\n",
            " 57% 38/67 [00:05<00:03,  8.74it/s]\u001b[A\n",
            " 58% 39/67 [00:05<00:03,  8.95it/s]\u001b[A\n",
            " 60% 40/67 [00:05<00:02,  9.01it/s]\u001b[A\n",
            " 61% 41/67 [00:05<00:02,  8.85it/s]\u001b[A\n",
            " 63% 42/67 [00:05<00:02,  9.04it/s]\u001b[A\n",
            " 64% 43/67 [00:05<00:02,  9.07it/s]\u001b[A\n",
            " 66% 44/67 [00:05<00:02,  8.99it/s]\u001b[A\n",
            " 67% 45/67 [00:05<00:02,  8.99it/s]\u001b[A\n",
            " 69% 46/67 [00:05<00:02,  9.06it/s]\u001b[A\n",
            " 70% 47/67 [00:06<00:02,  9.15it/s]\u001b[A\n",
            " 73% 49/67 [00:06<00:01,  9.25it/s]\u001b[A\n",
            " 75% 50/67 [00:06<00:01,  9.25it/s]\u001b[A\n",
            " 78% 52/67 [00:06<00:01,  9.43it/s]\u001b[A\n",
            " 79% 53/67 [00:06<00:01,  9.13it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  9.06it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  9.07it/s]\u001b[A\n",
            " 84% 56/67 [00:07<00:01,  9.16it/s]\u001b[A\n",
            " 85% 57/67 [00:07<00:01,  9.25it/s]\u001b[A\n",
            " 87% 58/67 [00:07<00:00,  9.32it/s]\u001b[A\n",
            " 88% 59/67 [00:07<00:00,  9.38it/s]\u001b[A\n",
            " 90% 60/67 [00:07<00:00,  9.38it/s]\u001b[A\n",
            " 91% 61/67 [00:07<00:00,  9.34it/s]\u001b[A\n",
            " 93% 62/67 [00:07<00:00,  9.26it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  8.98it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  8.95it/s]\u001b[A\n",
            " 97% 65/67 [00:08<00:00,  9.07it/s]\u001b[A\n",
            " 99% 66/67 [00:08<00:00,  9.13it/s]\u001b[A\n",
            "{'eval_loss': 0.32189732789993286, 'eval_bleu': 0.0, 'eval_accuracy': 0.6692, 'eval_gen_len': 2.0, 'eval_runtime': 8.4044, 'eval_samples_per_second': 63.3, 'eval_steps_per_second': 7.972, 'epoch': 4.22}\n",
            "\n",
            " 84% 4500/5335 [24:47<03:41,  3.77it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:31:04,583 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-4500\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:31:04,583 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:31:08,054 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:31:08,055 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:31:08,055 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:31:08,103 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-4500/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:31:15,975 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-3250] due to args.save_total_limit\n",
            "{'loss': 0.3662, 'learning_rate': 6.888472352389878e-06, 'epoch': 4.31}\n",
            "{'loss': 0.3542, 'learning_rate': 5.951265229615746e-06, 'epoch': 4.4}\n",
            " 89% 4750/5335 [26:01<02:24,  4.06it/s][INFO|trainer.py:2907] 2023-02-14 21:32:17,753 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:32:17,753 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:32:17,753 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 19.54it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.10it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.26it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:06,  9.15it/s]\u001b[A\n",
            " 13% 9/67 [00:00<00:06,  8.49it/s]\u001b[A\n",
            " 15% 10/67 [00:01<00:07,  7.56it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:07,  7.19it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:07,  7.14it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:07,  7.09it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:07,  7.24it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:07,  7.25it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:07,  7.13it/s]\u001b[A\n",
            " 25% 17/67 [00:02<00:07,  7.03it/s]\u001b[A\n",
            " 27% 18/67 [00:02<00:06,  7.14it/s]\u001b[A\n",
            " 28% 19/67 [00:02<00:06,  7.02it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:06,  6.99it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:06,  7.16it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:06,  7.20it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:06,  7.03it/s]\u001b[A\n",
            " 36% 24/67 [00:03<00:06,  6.93it/s]\u001b[A\n",
            " 37% 25/67 [00:03<00:06,  6.78it/s]\u001b[A\n",
            " 39% 26/67 [00:03<00:06,  6.60it/s]\u001b[A\n",
            " 40% 27/67 [00:03<00:06,  6.47it/s]\u001b[A\n",
            " 42% 28/67 [00:03<00:06,  6.48it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:05,  6.41it/s]\u001b[A\n",
            " 45% 30/67 [00:04<00:05,  6.43it/s]\u001b[A\n",
            " 46% 31/67 [00:04<00:05,  6.33it/s]\u001b[A\n",
            " 48% 32/67 [00:04<00:05,  6.11it/s]\u001b[A\n",
            " 49% 33/67 [00:04<00:05,  6.08it/s]\u001b[A\n",
            " 51% 34/67 [00:04<00:05,  6.16it/s]\u001b[A\n",
            " 52% 35/67 [00:04<00:05,  6.18it/s]\u001b[A\n",
            " 54% 36/67 [00:05<00:04,  6.65it/s]\u001b[A\n",
            " 55% 37/67 [00:05<00:04,  7.35it/s]\u001b[A\n",
            " 57% 38/67 [00:05<00:03,  7.80it/s]\u001b[A\n",
            " 58% 39/67 [00:05<00:03,  8.12it/s]\u001b[A\n",
            " 60% 40/67 [00:05<00:03,  8.39it/s]\u001b[A\n",
            " 61% 41/67 [00:05<00:03,  8.43it/s]\u001b[A\n",
            " 63% 42/67 [00:05<00:02,  8.56it/s]\u001b[A\n",
            " 66% 44/67 [00:05<00:02,  9.15it/s]\u001b[A\n",
            " 67% 45/67 [00:06<00:02,  9.14it/s]\u001b[A\n",
            " 69% 46/67 [00:06<00:02,  9.20it/s]\u001b[A\n",
            " 72% 48/67 [00:06<00:01,  9.51it/s]\u001b[A\n",
            " 73% 49/67 [00:06<00:01,  9.34it/s]\u001b[A\n",
            " 75% 50/67 [00:06<00:01,  9.40it/s]\u001b[A\n",
            " 76% 51/67 [00:06<00:01,  9.22it/s]\u001b[A\n",
            " 79% 53/67 [00:06<00:01,  9.34it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:01,  9.18it/s]\u001b[A\n",
            " 82% 55/67 [00:07<00:01,  9.32it/s]\u001b[A\n",
            " 84% 56/67 [00:07<00:01,  8.99it/s]\u001b[A\n",
            " 85% 57/67 [00:07<00:01,  9.16it/s]\u001b[A\n",
            " 87% 58/67 [00:07<00:00,  9.33it/s]\u001b[A\n",
            " 88% 59/67 [00:07<00:00,  9.01it/s]\u001b[A\n",
            " 90% 60/67 [00:07<00:00,  9.09it/s]\u001b[A\n",
            " 91% 61/67 [00:07<00:00,  9.23it/s]\u001b[A\n",
            " 93% 62/67 [00:07<00:00,  9.30it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  9.37it/s]\u001b[A\n",
            " 96% 64/67 [00:08<00:00,  9.50it/s]\u001b[A\n",
            " 97% 65/67 [00:08<00:00,  8.88it/s]\u001b[A\n",
            "100% 67/67 [00:08<00:00,  9.10it/s]\u001b[A\n",
            "{'eval_loss': 0.31689420342445374, 'eval_bleu': 0.0, 'eval_accuracy': 0.6692, 'eval_gen_len': 2.0, 'eval_runtime': 8.5662, 'eval_samples_per_second': 62.105, 'eval_steps_per_second': 7.821, 'epoch': 4.45}\n",
            "\n",
            " 89% 4750/5335 [26:09<02:24,  4.06it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:32:26,320 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-4750\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:32:26,321 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4750/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:32:29,827 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4750/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:32:29,828 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:32:29,828 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-4750/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:32:29,877 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-4750/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:32:37,726 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-3500] due to args.save_total_limit\n",
            "{'loss': 0.3604, 'learning_rate': 5.014058106841612e-06, 'epoch': 4.5}\n",
            "{'loss': 0.3625, 'learning_rate': 4.076850984067479e-06, 'epoch': 4.59}\n",
            "{'loss': 0.3548, 'learning_rate': 3.1396438612933463e-06, 'epoch': 4.69}\n",
            " 94% 5000/5335 [27:23<01:19,  4.24it/s][INFO|trainer.py:2907] 2023-02-14 21:33:40,428 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:33:40,428 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:33:40,428 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 16.61it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.39it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.31it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:05,  9.92it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.67it/s]\u001b[A\n",
            " 16% 11/67 [00:01<00:05,  9.72it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.51it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.28it/s]\u001b[A\n",
            " 21% 14/67 [00:01<00:05,  9.36it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.48it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.18it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  9.12it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  9.29it/s]\u001b[A\n",
            " 28% 19/67 [00:01<00:05,  9.19it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.34it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.24it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.18it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.32it/s]\u001b[A\n",
            " 37% 25/67 [00:02<00:04,  9.40it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.49it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.51it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.45it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:03,  9.46it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:03,  9.48it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  9.41it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.44it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.53it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.29it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.47it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  7.82it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  7.44it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  7.14it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:03,  7.14it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:03,  7.03it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:03,  7.15it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:03,  7.20it/s]\u001b[A\n",
            " 66% 44/67 [00:04<00:03,  7.07it/s]\u001b[A\n",
            " 67% 45/67 [00:05<00:03,  7.03it/s]\u001b[A\n",
            " 69% 46/67 [00:05<00:02,  7.09it/s]\u001b[A\n",
            " 70% 47/67 [00:05<00:02,  7.09it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:02,  6.96it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:02,  7.00it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:02,  7.07it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:02,  6.80it/s]\u001b[A\n",
            " 78% 52/67 [00:06<00:02,  6.85it/s]\u001b[A\n",
            " 79% 53/67 [00:06<00:02,  6.83it/s]\u001b[A\n",
            " 81% 54/67 [00:06<00:02,  6.30it/s]\u001b[A\n",
            " 82% 55/67 [00:06<00:01,  6.20it/s]\u001b[A\n",
            " 84% 56/67 [00:06<00:01,  6.17it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  6.16it/s]\u001b[A\n",
            " 87% 58/67 [00:07<00:01,  5.98it/s]\u001b[A\n",
            " 88% 59/67 [00:07<00:01,  5.92it/s]\u001b[A\n",
            " 90% 60/67 [00:07<00:01,  5.78it/s]\u001b[A\n",
            " 91% 61/67 [00:07<00:01,  5.82it/s]\u001b[A\n",
            " 93% 62/67 [00:07<00:00,  5.67it/s]\u001b[A\n",
            " 94% 63/67 [00:07<00:00,  6.00it/s]\u001b[A\n",
            " 96% 64/67 [00:08<00:00,  6.80it/s]\u001b[A\n",
            " 97% 65/67 [00:08<00:00,  7.33it/s]\u001b[A\n",
            " 99% 66/67 [00:08<00:00,  7.77it/s]\u001b[A\n",
            "100% 67/67 [00:08<00:00,  8.07it/s]\u001b[A\n",
            "{'eval_loss': 0.31713852286338806, 'eval_bleu': 0.0, 'eval_accuracy': 0.6485, 'eval_gen_len': 2.0, 'eval_runtime': 8.5776, 'eval_samples_per_second': 62.022, 'eval_steps_per_second': 7.811, 'epoch': 4.69}\n",
            "\n",
            " 94% 5000/5335 [27:32<01:19,  4.24it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:33:49,007 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-5000\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:33:49,008 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:33:52,467 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:33:52,468 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:33:52,469 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:33:52,513 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-5000/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:34:00,332 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-3750] due to args.save_total_limit\n",
            "{'loss': 0.3551, 'learning_rate': 2.202436738519213e-06, 'epoch': 4.78}\n",
            "{'loss': 0.3697, 'learning_rate': 1.2652296157450795e-06, 'epoch': 4.87}\n",
            " 98% 5250/5335 [28:45<00:20,  4.20it/s][INFO|trainer.py:2907] 2023-02-14 21:35:02,472 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:35:02,472 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:35:02,473 >>   Batch size = 8\n",
            "\n",
            "  0% 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/67 [00:00<00:03, 18.46it/s]\u001b[A\n",
            "  6% 4/67 [00:00<00:05, 11.48it/s]\u001b[A\n",
            "  9% 6/67 [00:00<00:05, 10.28it/s]\u001b[A\n",
            " 12% 8/67 [00:00<00:05,  9.86it/s]\u001b[A\n",
            " 15% 10/67 [00:00<00:05,  9.77it/s]\u001b[A\n",
            " 18% 12/67 [00:01<00:05,  9.60it/s]\u001b[A\n",
            " 19% 13/67 [00:01<00:05,  9.64it/s]\u001b[A\n",
            " 22% 15/67 [00:01<00:05,  9.51it/s]\u001b[A\n",
            " 24% 16/67 [00:01<00:05,  9.51it/s]\u001b[A\n",
            " 25% 17/67 [00:01<00:05,  9.40it/s]\u001b[A\n",
            " 27% 18/67 [00:01<00:05,  9.34it/s]\u001b[A\n",
            " 30% 20/67 [00:02<00:04,  9.58it/s]\u001b[A\n",
            " 31% 21/67 [00:02<00:04,  9.56it/s]\u001b[A\n",
            " 33% 22/67 [00:02<00:04,  9.63it/s]\u001b[A\n",
            " 34% 23/67 [00:02<00:04,  9.41it/s]\u001b[A\n",
            " 36% 24/67 [00:02<00:04,  9.26it/s]\u001b[A\n",
            " 39% 26/67 [00:02<00:04,  9.37it/s]\u001b[A\n",
            " 40% 27/67 [00:02<00:04,  9.39it/s]\u001b[A\n",
            " 42% 28/67 [00:02<00:04,  9.10it/s]\u001b[A\n",
            " 43% 29/67 [00:03<00:04,  8.97it/s]\u001b[A\n",
            " 45% 30/67 [00:03<00:04,  9.04it/s]\u001b[A\n",
            " 46% 31/67 [00:03<00:04,  8.99it/s]\u001b[A\n",
            " 48% 32/67 [00:03<00:03,  8.97it/s]\u001b[A\n",
            " 49% 33/67 [00:03<00:03,  9.20it/s]\u001b[A\n",
            " 51% 34/67 [00:03<00:03,  9.12it/s]\u001b[A\n",
            " 52% 35/67 [00:03<00:03,  9.11it/s]\u001b[A\n",
            " 54% 36/67 [00:03<00:03,  8.72it/s]\u001b[A\n",
            " 55% 37/67 [00:03<00:03,  8.94it/s]\u001b[A\n",
            " 57% 38/67 [00:04<00:03,  8.99it/s]\u001b[A\n",
            " 58% 39/67 [00:04<00:03,  9.22it/s]\u001b[A\n",
            " 60% 40/67 [00:04<00:03,  8.99it/s]\u001b[A\n",
            " 61% 41/67 [00:04<00:02,  8.98it/s]\u001b[A\n",
            " 63% 42/67 [00:04<00:02,  9.16it/s]\u001b[A\n",
            " 64% 43/67 [00:04<00:02,  8.96it/s]\u001b[A\n",
            " 67% 45/67 [00:04<00:02,  9.41it/s]\u001b[A\n",
            " 69% 46/67 [00:04<00:02,  9.24it/s]\u001b[A\n",
            " 72% 48/67 [00:05<00:01,  9.64it/s]\u001b[A\n",
            " 73% 49/67 [00:05<00:01,  9.33it/s]\u001b[A\n",
            " 75% 50/67 [00:05<00:01,  9.40it/s]\u001b[A\n",
            " 76% 51/67 [00:05<00:01,  9.32it/s]\u001b[A\n",
            " 78% 52/67 [00:05<00:01,  9.41it/s]\u001b[A\n",
            " 79% 53/67 [00:05<00:01,  9.35it/s]\u001b[A\n",
            " 81% 54/67 [00:05<00:01,  9.16it/s]\u001b[A\n",
            " 82% 55/67 [00:05<00:01,  9.25it/s]\u001b[A\n",
            " 84% 56/67 [00:05<00:01,  8.62it/s]\u001b[A\n",
            " 85% 57/67 [00:06<00:01,  8.05it/s]\u001b[A\n",
            " 87% 58/67 [00:06<00:01,  7.68it/s]\u001b[A\n",
            " 88% 59/67 [00:06<00:01,  7.29it/s]\u001b[A\n",
            " 90% 60/67 [00:06<00:00,  7.31it/s]\u001b[A\n",
            " 91% 61/67 [00:06<00:00,  7.18it/s]\u001b[A\n",
            " 93% 62/67 [00:06<00:00,  7.27it/s]\u001b[A\n",
            " 94% 63/67 [00:06<00:00,  7.25it/s]\u001b[A\n",
            " 96% 64/67 [00:07<00:00,  7.31it/s]\u001b[A\n",
            " 97% 65/67 [00:07<00:00,  7.33it/s]\u001b[A\n",
            " 99% 66/67 [00:07<00:00,  7.17it/s]\u001b[A\n",
            "100% 67/67 [00:07<00:00,  7.26it/s]\u001b[A\n",
            "{'eval_loss': 0.3129732310771942, 'eval_bleu': 0.0, 'eval_accuracy': 0.7011, 'eval_gen_len': 2.0, 'eval_runtime': 7.7294, 'eval_samples_per_second': 68.828, 'eval_steps_per_second': 8.668, 'epoch': 4.92}\n",
            "\n",
            " 98% 5250/5335 [28:53<00:20,  4.20it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2656] 2023-02-14 21:35:10,203 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base/checkpoint-5250\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:35:10,205 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-5250/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:35:13,707 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-5250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:35:13,708 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-5250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:35:13,709 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/checkpoint-5250/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:35:13,757 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/checkpoint-5250/spiece.model\n",
            "[INFO|trainer.py:2734] 2023-02-14 21:35:21,611 >> Deleting older checkpoint [out/tomatoes/t5_v1_1_freeze_base/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 0.344, 'learning_rate': 3.280224929709466e-07, 'epoch': 4.97}\n",
            "100% 5335/5335 [29:26<00:00,  4.17it/s][INFO|trainer.py:1852] 2023-02-14 21:35:43,273 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1946] 2023-02-14 21:35:43,273 >> Loading best model from out/tomatoes/t5_v1_1_freeze_base/checkpoint-5250 (score: 0.7011).\n",
            "{'train_runtime': 1767.5451, 'train_samples_per_second': 24.13, 'train_steps_per_second': 3.018, 'train_loss': 1.0119431441145292, 'epoch': 5.0}\n",
            "100% 5335/5335 [29:27<00:00,  3.02it/s]\n",
            "[INFO|trainer.py:2656] 2023-02-14 21:35:44,156 >> Saving model checkpoint to out/tomatoes/t5_v1_1_freeze_base\n",
            "[INFO|configuration_utils.py:447] 2023-02-14 21:35:44,157 >> Configuration saved in out/tomatoes/t5_v1_1_freeze_base/config.json\n",
            "[INFO|modeling_utils.py:1624] 2023-02-14 21:35:47,639 >> Model weights saved in out/tomatoes/t5_v1_1_freeze_base/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2023-02-14 21:35:47,641 >> tokenizer config file saved in out/tomatoes/t5_v1_1_freeze_base/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2023-02-14 21:35:47,641 >> Special tokens file saved in out/tomatoes/t5_v1_1_freeze_base/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:187] 2023-02-14 21:35:47,686 >> Copy vocab file to out/tomatoes/t5_v1_1_freeze_base/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     1.0119\n",
            "  train_runtime            = 0:29:27.54\n",
            "  train_samples            =       8530\n",
            "  train_samples_per_second =      24.13\n",
            "  train_steps_per_second   =      3.018\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2907] 2023-02-14 21:35:47,697 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:35:47,697 >>   Num examples = 532\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:35:47,697 >>   Batch size = 8\n",
            "100% 67/67 [00:08<00:00,  7.74it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.7011\n",
            "  eval_bleu               =        0.0\n",
            "  eval_gen_len            =        2.0\n",
            "  eval_loss               =      0.313\n",
            "  eval_runtime            = 0:00:08.81\n",
            "  eval_samples            =        532\n",
            "  eval_samples_per_second =     60.341\n",
            "  eval_steps_per_second   =      7.599\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:2907] 2023-02-14 21:35:56,517 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2909] 2023-02-14 21:35:56,517 >>   Num examples = 534\n",
            "[INFO|trainer.py:2912] 2023-02-14 21:35:56,518 >>   Batch size = 8\n",
            "100% 67/67 [00:07<00:00,  9.07it/s]\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.7172\n",
            "  predict_bleu               =        0.0\n",
            "  predict_gen_len            =        2.0\n",
            "  predict_loss               =     0.3123\n",
            "  predict_runtime            = 0:00:07.52\n",
            "  predict_samples            =        534\n",
            "  predict_samples_per_second =     71.009\n",
            "  predict_steps_per_second   =      8.909\n",
            "[INFO|modelcard.py:444] 2023-02-14 21:36:04,439 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 0.0}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7011}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_83BBoqK3C5s",
        "outputId": "46efb654-2e7a-4f1b-d5ca-ab2d746c463b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.13.1+cu116)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.4.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate) (4.4.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/out/tomatoes/t5_v1_1_freeze_base'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(path, torch_dtype=torch.float32)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "model_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "SxC2abIlGimV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'This movie sucks!'\n",
        "text_p = model_pipeline(text, max_length=60)\n",
        "text_p"
      ],
      "metadata": {
        "id": "VfVDWOe8GikE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50b2e57-2ad7-411b-b863-df5602302973"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'negative'}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Wow, it's amazing.\"\n",
        "text_p = model_pipeline(text, max_length=60)\n",
        "text_p"
      ],
      "metadata": {
        "id": "nnODmSSxGihs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36018de5-18d8-49a5-b7fa-f42ab7a81395"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'positive'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "static_que = ' Is this a positive or negative review?'\n",
        "\n",
        "def eval(dataset):\n",
        "  result = []\n",
        "\n",
        "  for text in tqdm(dataset):\n",
        "    text_q = text + static_que\n",
        "    prediction = model_pipeline(text_q, max_length=60)\n",
        "    result.append(prediction[0]['generated_text'])\n",
        "  return result"
      ],
      "metadata": {
        "id": "Q99nxa-oGifT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('rotten_tomatoes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "e1f80a3d05274155a35f9def87ed8348",
            "bbd18c87c04c45a5a019a9eb55ec3e8d",
            "b4ee180a669d4461b7dd553116797cc6",
            "d669babde89e4300a91836538ae75c52",
            "c39e56a0a6a1439082de1f3cde5b9a67",
            "b806d943f9bc4c0bb94cf7c1af2fb319",
            "384eb483f76342cc85feea6086586644",
            "8bd35cae3d6e4058a4025a2f39b20787",
            "386f48b4cd1e4dc39709712a875ed074",
            "e25a74da0f8443869a91c812a2579ef3",
            "25e6f3546adf43b7b6f1101819797abd"
          ]
        },
        "id": "RaXAg-xgxM40",
        "outputId": "6b6c908d-8835-4dd0-aea7-716dc2a4d09f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset rotten_tomatoes (/root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1f80a3d05274155a35f9def87ed8348"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = eval(dataset['test']['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbGQvQr7xD-L",
        "outputId": "5aa4c715-10c8-4991-9554-54e0fe6140c7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1066/1066 [06:41<00:00,  2.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {\n",
        "    'negative': 0,\n",
        "    'positive': 1\n",
        "}\n",
        "\n",
        "predictions = [label2id[prediction] for prediction in predictions]"
      ],
      "metadata": {
        "id": "MvBfGCwj3jjh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(dataset['test']['label'], predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VnZcb3Nxi2Q",
        "outputId": "4b4bce3f-5575-45fc-b400-b62c6006b01e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6557223264540337"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}